{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29c2d300",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85451903",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "454b602f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import json\n",
    "import tensorflow as tf\n",
    "import gc\n",
    "from os import mkdir\n",
    "\n",
    "import math\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import layers, models\n",
    "import tensorflow_addons as tfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "debf677c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (r\"C:\\Users\\chuqi\\ac215\\kaggle-data\\character_to_prediction_index.json\", \"r\") as f:\n",
    "    char_to_num = json.load(f)\n",
    "    \n",
    "pad_token = '^'\n",
    "pad_token_idx = 59\n",
    "\n",
    "char_to_num[pad_token] = pad_token_idx\n",
    "num_to_char = {j:i for i,j in char_to_num.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "917fe5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"how2sign_realigned_test.csv\",index_col=0)\n",
    "df['FILE_ID'] = df['SENTENCE_NAME'].apply(lambda x: \"preprocessed_\"+x+'.npy') \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa7467b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read capy csv \n",
    "df_pre = pd.read_csv(r\"C:/Users/chuqi/ac215/capy-data/test/preprocessed__fZbAxSSbX4_0-5-rgb_front.csv\",index_col=0)\n",
    "cols = df_pre.columns.tolist()[2:]\n",
    "cols[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a1aa460",
   "metadata": {},
   "outputs": [],
   "source": [
    "FACE = [\n",
    "    61, 185, 40, 39, 37, 0, 267, 269, 270, 409,\n",
    "    291, 146, 91, 181, 84, 17, 314, 405, 321, 375,\n",
    "    78, 191, 80, 81, 82, 13, 312, 311, 310, 415,\n",
    "    95, 88, 178, 87, 14, 317, 402, 318, 324, 308,\n",
    "]\n",
    "\n",
    "LPOSE = [13, 15, 17, 19, 21]\n",
    "RPOSE = [14, 16, 18, 20, 22]\n",
    "POSE = LPOSE + RPOSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f879002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FACE = [int(cor.split(\"_\")[-1]) for cor in cols if 'x_face' in cor]\n",
    "HAND = [int(cor.split(\"_\")[-1]) for cor in cols if 'x_right_hand' in cor]\n",
    "# POSE = [int(cor.split(\"_\")[-1]) for cor in cols if 'x_pose' in cor]\n",
    "# LPOSE = [i for i in POSE if i%2!=0]\n",
    "# RPOSE = [i for i in POSE if i%2==0]\n",
    "\n",
    "LPOSE_cols_x = ['x'+'_pose_'+str(i) for i in LPOSE]\n",
    "LPOSE_cols_y = ['y'+'_pose_'+str(i) for i in LPOSE]\n",
    "LPOSE_cols_z = ['z'+'_pose_'+str(i) for i in LPOSE]\n",
    "\n",
    "RPOSE_cols_x = ['x'+'_pose_'+str(i) for i in RPOSE]\n",
    "RPOSE_cols_y = ['y'+'_pose_'+str(i) for i in RPOSE]\n",
    "RPOSE_cols_z = ['z'+'_pose_'+str(i) for i in RPOSE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b36b474",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# FACE\n",
    "FACE_IDX_X = [i for i, col in enumerate(cols) if 'face' in col and \"x\" in col and int(col[7:]) in FACE]\n",
    "FACE_IDX_Y = [i for i, col in enumerate(cols) if 'face' in col and \"y\" in col and int(col[7:]) in FACE]\n",
    "FACE_IDX_Z = [i for i, col in enumerate(cols) if 'face' in col and \"z\" in col and int(col[7:]) in FACE]\n",
    "\n",
    "# HAND \n",
    "RHAND_IDX_X = [i for i, col in enumerate(cols) if 'x_right_hand' in col]\n",
    "LHAND_IDX_X = [i for i, col in enumerate(cols) if 'x_left_hand' in col]\n",
    "\n",
    "RHAND_IDX_Y = [i for i, col in enumerate(cols) if 'y_right_hand' in col]\n",
    "LHAND_IDX_Y = [i for i, col in enumerate(cols) if 'y_left_hand' in col]\n",
    "\n",
    "RHAND_IDX_Z = [i for i, col in enumerate(cols) if 'z_right_hand' in col]\n",
    "LHAND_IDX_Z = [i for i, col in enumerate(cols) if 'z_left_hand' in col]\n",
    "\n",
    "# POSE RIGHT\n",
    "RPOSE_IDX_X = [i for i, col in enumerate(cols) if col in RPOSE_cols_x]\n",
    "RPOSE_IDX_Y = [i for i, col in enumerate(cols) if col in RPOSE_cols_y]\n",
    "RPOSE_IDX_Z = [i for i, col in enumerate(cols) if col in RPOSE_cols_z]\n",
    "\n",
    "# POSE LEFT\n",
    "LPOSE_IDX_X = [i for i, col in enumerate(cols) if col in LPOSE_cols_x]\n",
    "LPOSE_IDX_Y = [i for i, col in enumerate(cols) if col in LPOSE_cols_y]\n",
    "LPOSE_IDX_Z = [i for i, col in enumerate(cols) if col in LPOSE_cols_z]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02be82ef",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def load_npy_data(file_id):\n",
    "    folder = \"C:/Users/chuqi/ac215/capy-data/test_npy\"\n",
    "    file_name = os.path.join(folder,file_id)\n",
    "    # Delete the first 3 columns: index, file_name, frame num\n",
    "    return np.load(file_name,allow_pickle=True)[:,3:]\n",
    "\n",
    "def fmkdir(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.mkdir(path)\n",
    "\n",
    "folder =\"C:/Users/chuqi/ac215/capy_data_test/test_npy\"\n",
    "file_ls = os.listdir(folder)\n",
    "for file_id in tqdm.tqdm(file_ls):\n",
    "    frames = load_npy_data(file_id)\n",
    "    phrase = str(df[df['FILE_ID']==file_id]['SENTENCE'].iloc[0])\n",
    "    \n",
    "    def process(x):\n",
    "        face_x = x[:, FACE_IDX_X]\n",
    "        face_y = x[:, FACE_IDX_Y]\n",
    "        face_z = x[:, FACE_IDX_Z]\n",
    "\n",
    "        rhand_x = x[:, RHAND_IDX_X]\n",
    "        rhand_y = x[:, RHAND_IDX_Y]\n",
    "        rhand_z = x[:, RHAND_IDX_Z]\n",
    "        \n",
    "        lhand_x = x[:, LHAND_IDX_X]\n",
    "        lhand_y = x[:, LHAND_IDX_Y]\n",
    "        lhand_z = x[:, LHAND_IDX_Z]\n",
    "\n",
    "        rpose_x = x[:, RPOSE_IDX_X]\n",
    "        rpose_y = x[:, RPOSE_IDX_Y]\n",
    "        rpose_z = x[:, RPOSE_IDX_Z]\n",
    "        \n",
    "        lpose_x = x[:, LPOSE_IDX_X]\n",
    "        lpose_y = x[:, LPOSE_IDX_Y]\n",
    "        lpose_z = x[:, LPOSE_IDX_Z]\n",
    "                \n",
    "        rhnonans = ~np.isnan(np.sum(rhand_x, axis=1).astype(float))\n",
    "        lhnonans = ~np.isnan(np.sum(lhand_x, axis=1).astype(float))\n",
    "        rphonans = ~np.isnan(np.sum(rpose_x, axis=1).astype(float))\n",
    "        lphonans = ~np.isnan(np.sum(lpose_x, axis=1).astype(float))\n",
    "        fcnonans = ~np.isnan(np.sum(face_x,   axis=1).astype(float))\n",
    "        \n",
    "        rhand = np.stack([rhand_x, rhand_y, rhand_z], axis=-1)[rhnonans]\n",
    "        rpose = np.stack([rpose_x, rpose_y, rpose_z], axis=-1)[rphonans]\n",
    "        lhand = np.stack([lhand_x, lhand_y, lhand_z], axis=-1)[lhnonans]\n",
    "        lpose = np.stack([lpose_x, lpose_y, lpose_z], axis=-1)[lphonans]\n",
    "        face = np.stack([face_x, face_y, face_z], axis=-1)[fcnonans]\n",
    "\n",
    "        return rhand, lhand, rpose, lpose, face\n",
    "\n",
    "    rhand, lhand, rpose, lpose, face = process(frames)\n",
    "    print(rhand.shape, lhand.shape, rpose.shape, lpose.shape, face.shape)\n",
    "    \n",
    "    def gen(df):\n",
    "        # extract numpy file \n",
    "        folder_directory = r\"C:/Users/chuqi/ac215/capy_data_test/test_npy\"\n",
    "        file_ls = [f for f in os.listdir(folder_directory) if '.npy' in f]\n",
    "        for file_id in file_ls:\n",
    "            try:\n",
    "                # pqfile = f\"/kaggle/input/asl-fingerspelling/train_landmarks/{file_id}.parquet\"\n",
    "                npy_file = f\"C:/Users/chuqi/ac215/capy-data/test_npy/{file_id}\"\n",
    "                frames = load_npy_data(file_id)\n",
    "                phrase = df[df['FILE_ID']==file_id][\"SENTENCE\"].iloc[0]\n",
    "                \n",
    "                # Preprocess\n",
    "                rhand, lhand, rpose, lpose, face = process(frames)\n",
    "                if max(rhand.shape[0], lhand.shape[0]) > len(phrase):\n",
    "                    yield rhand, lhand, rpose, lpose, face\n",
    "            except:\n",
    "                print(\"Error file:\", file_id)\n",
    "    RHAND = []\n",
    "    LHAND = []\n",
    "    RPOSE = []\n",
    "    LPOSE = []\n",
    "    FACE = []\n",
    "\n",
    "    for rhand, lhand, rpose, lpose, face in gen(df):\n",
    "        RHAND.extend(rhand)\n",
    "        LHAND.extend(lhand)\n",
    "        RPOSE.extend(rpose)\n",
    "        LPOSE.extend(lpose)\n",
    "        FACE.extend(face)\n",
    "        \n",
    "    RHAND = np.array(RHAND,dtype=np.float64)\n",
    "    LHAND = np.array(LHAND,dtype=np.float64)\n",
    "    RPOSE = np.array(RPOSE,dtype=np.float64)\n",
    "    LPOSE = np.array(LPOSE,dtype=np.float64)\n",
    "    FACE = np.array(FACE, dtype=np.float64)\n",
    "    gc.collect()\n",
    "\n",
    "    RHM = np.mean(RHAND, axis=0)\n",
    "    LHM = np.mean(LHAND, axis=0)\n",
    "    RPM = np.mean(RPOSE, axis=0)\n",
    "    LPM = np.mean(LPOSE, axis=0)\n",
    "    FACEM = np.mean(FACE, axis=0)\n",
    "\n",
    "    RHS = np.std(RHAND, axis=0)\n",
    "    LHS = np.std(LHAND, axis=0)\n",
    "    RPS = np.std(RPOSE, axis=0)\n",
    "    LPS = np.std(LPOSE, axis=0)\n",
    "    FACES = np.std(FACE, axis=0)\n",
    "\n",
    "    # npyPath = \"kaggle/input/aslfr-dataset-tfrecords\"\n",
    "    npyPath = r\"C:/Users/chuqi/ac215/kaggle-data/capy_tfds/mean_std\"\n",
    "    fmkdir(os.path.join(npyPath,\"mean_std\"))\n",
    "    np.save(os.path.join(npyPath,\"mean_std/rh_mean.npy\"), RHM)\n",
    "    np.save(os.path.join(npyPath,\"mean_std/lh_mean.npy\"), LHM)\n",
    "    np.save(os.path.join(npyPath,\"mean_std/rp_mean.npy\"), RPM)\n",
    "    np.save(os.path.join(npyPath,\"mean_std/lp_mean.npy\"), LPM)\n",
    "    np.save(os.path.join(npyPath,\"mean_std/face_mean.npy\"), FACEM)\n",
    "\n",
    "    np.save(os.path.join(npyPath,\"mean_std/rh_std.npy\"), RHS)\n",
    "    np.save(os.path.join(npyPath,\"mean_std/lh_std.npy\"), LHS)\n",
    "    np.save(os.path.join(npyPath,\"mean_std/rp_std.npy\"), RPS)\n",
    "    np.save(os.path.join(npyPath,\"mean_std/lp_std.npy\"), LPS)\n",
    "    np.save(os.path.join(npyPath,\"mean_std/face_std.npy\"), FACES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d92de00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FRAME_LEN = 128\n",
    "FRAME_LEN = 128*2\n",
    "\n",
    "@tf.function()\n",
    "def resize_pad(x):\n",
    "    if tf.shape(x)[0] < FRAME_LEN:\n",
    "        x = tf.pad(x, ([[0, FRAME_LEN-tf.shape(x)[0]], [0, 0], [0, 0]]), constant_values=float(\"NaN\"))\n",
    "    else:\n",
    "        x = tf.image.resize(x, (FRAME_LEN, tf.shape(x)[1]))\n",
    "    return x\n",
    "\n",
    "@tf.function(jit_compile=True)\n",
    "def pre_process0(x):    \n",
    "    face_x = tf.gather(x, FACE_IDX_X, axis=1)\n",
    "    face_y = tf.gather(x, FACE_IDX_Y, axis=1)\n",
    "    face_z = tf.gather(x, FACE_IDX_Z, axis=1)\n",
    "\n",
    "    rhand_x = tf.gather(x, RHAND_IDX_X, axis=1)\n",
    "    rhand_y = tf.gather(x, RHAND_IDX_Y, axis=1)\n",
    "    rhand_z = tf.gather(x, RHAND_IDX_Z, axis=1)\n",
    "    \n",
    "    lhand_x = tf.gather(x, LHAND_IDX_X, axis=1)\n",
    "    lhand_y = tf.gather(x, LHAND_IDX_Y, axis=1)\n",
    "    lhand_z = tf.gather(x, LHAND_IDX_Z, axis=1)\n",
    "\n",
    "    rpose_x = tf.gather(x, RPOSE_IDX_X, axis=1)\n",
    "    rpose_y = tf.gather(x, RPOSE_IDX_Y, axis=1)\n",
    "    rpose_z = tf.gather(x, RPOSE_IDX_Z, axis=1)\n",
    "    \n",
    "    lpose_x = tf.gather(x, LPOSE_IDX_X, axis=1)\n",
    "    lpose_y = tf.gather(x, LPOSE_IDX_Y, axis=1)\n",
    "    lpose_z = tf.gather(x, LPOSE_IDX_Z, axis=1)\n",
    "    \n",
    "    face   = tf.concat([face_x[..., tf.newaxis], face_y[..., tf.newaxis], face_z[..., tf.newaxis]], axis=-1)\n",
    "    rhand = tf.concat([rhand_x[..., tf.newaxis], rhand_y[..., tf.newaxis], rhand_z[..., tf.newaxis]], axis=-1)\n",
    "    lhand = tf.concat([lhand_x[..., tf.newaxis], lhand_y[..., tf.newaxis], lhand_z[..., tf.newaxis]], axis=-1)\n",
    "    rpose = tf.concat([rpose_x[..., tf.newaxis], rpose_y[..., tf.newaxis], rpose_z[..., tf.newaxis]], axis=-1)\n",
    "    lpose = tf.concat([lpose_x[..., tf.newaxis], lpose_y[..., tf.newaxis], lpose_z[..., tf.newaxis]], axis=-1)\n",
    "    \n",
    "    hand = tf.concat([rhand, lhand], axis=1)\n",
    "    hand = tf.where(tf.math.is_nan(hand), 0.0, hand)\n",
    "    mask = tf.math.not_equal(tf.reduce_sum(hand, axis=[1, 2]), 0.0)\n",
    "    \n",
    "\n",
    "    face = face[mask]\n",
    "    rhand = rhand[mask]\n",
    "    lhand = lhand[mask]\n",
    "    rpose = rpose[mask]\n",
    "    lpose = lpose[mask]\n",
    "\n",
    "    return face, rhand, lhand, rpose, lpose\n",
    "\n",
    "@tf.function()\n",
    "def pre_process1(face, rhand, lhand, rpose, lpose):\n",
    "    print(type(face),face.shape)\n",
    "    face   = (resize_pad(face) - FACEM) / FACES\n",
    "    rhand = (resize_pad(rhand) - RHM) / RHS\n",
    "    lhand = (resize_pad(lhand) - LHM) / LHS\n",
    "    rpose = (resize_pad(rpose) - RPM) / RPS\n",
    "    lpose = (resize_pad(lpose) - LPM) / LPS\n",
    "\n",
    "    x = tf.concat([face, rhand, lhand, rpose, lpose], axis=1)\n",
    "    s = tf.shape(x)\n",
    "    x = tf.reshape(x, (s[0], s[1]*s[2]))\n",
    "    x = tf.where(tf.math.is_nan(x), 0.0, x)\n",
    "    return x\n",
    "\n",
    "# frames = tf.cast(frames, dtype=tf.float32)\n",
    "frames_convert = frames.astype(float)\n",
    "frames_tf = tf.cast(frames_convert, dtype=tf.float32)\n",
    "pre0 = pre_process0(frames_tf)\n",
    "pre1 = pre_process1(*pre0)\n",
    "INPUT_SHAPE = list(pre1.shape)\n",
    "print(INPUT_SHAPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d2bd043a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Build TFrecords\n",
    "isGenTFrecord = True\n",
    "if isGenTFrecord:\n",
    "    fmkdir(r\"C:/Users/chuqi/ac215/capy_data_test/test_tfds\")\n",
    "    for file_id in tqdm.tqdm(os.listdir('C:/Users/chuqi/ac215/capy_data_test/test_npy')):\n",
    "        tffile = f\"C:/Users/chuqi/ac215/capy_data_test/test_tfds/{file_id}.tfrecord\"\n",
    "        frames = load_npy_data(file_id)\n",
    "        phrase = str(df[df['FILE_ID']==file_id]['SENTENCE'].iloc[0])\n",
    "        frames_convert = frames.astype(float)\n",
    "        frames_tf = tf.cast(frames_convert, dtype=tf.float32)\n",
    "        print(frames.shape, phrase)\n",
    "        \n",
    "        with tf.io.TFRecordWriter(tffile) as file_writer:\n",
    "            face, rhand, lhand, rpose, lpose = pre_process0(frames_tf)\n",
    "\n",
    "            if max(rhand.shape[0], lhand.shape[0]) < len(phrase):\n",
    "                continue\n",
    "                    \n",
    "            features = {}\n",
    "            features[\"face\"] = tf.train.Feature(float_list=tf.train.FloatList(value=tf.reshape(face, -1).numpy())) \n",
    "            features[\"rhand\"] = tf.train.Feature(float_list=tf.train.FloatList(value=tf.reshape(rhand, -1).numpy())) \n",
    "            features[\"lhand\"] = tf.train.Feature(float_list=tf.train.FloatList(value=tf.reshape(lhand, -1).numpy())) \n",
    "            features[\"rpose\"] = tf.train.Feature(float_list=tf.train.FloatList(value=tf.reshape(rpose, -1).numpy())) \n",
    "            features[\"lpose\"] = tf.train.Feature(float_list=tf.train.FloatList(value=tf.reshape(lpose, -1).numpy())) \n",
    "            features[\"phrase\"] = tf.train.Feature(int64_list=tf.train.Int64List(value=[char_to_num[x] for x in phrase]))\n",
    "                \n",
    "            record_bytes = tf.train.Example(features=tf.train.Features(feature=features)).SerializeToString()\n",
    "            file_writer.write(record_bytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e29bbeb7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def decode_fn(record_bytes):\n",
    "    schema = {\n",
    "        \"face\": tf.io.VarLenFeature(tf.float32),\n",
    "        \"rhand\": tf.io.VarLenFeature(tf.float32),\n",
    "        \"lhand\": tf.io.VarLenFeature(tf.float32),\n",
    "        \"rpose\": tf.io.VarLenFeature(tf.float32),\n",
    "        \"lpose\": tf.io.VarLenFeature(tf.float32),\n",
    "        \"phrase\": tf.io.VarLenFeature(tf.int64)\n",
    "    }\n",
    "    x = tf.io.parse_single_example(record_bytes, schema)\n",
    "\n",
    "    face = tf.reshape(tf.sparse.to_dense(x[\"face\"]), (-1, 40, 3))\n",
    "    rhand = tf.reshape(tf.sparse.to_dense(x[\"rhand\"]), (-1, 21, 3))\n",
    "    lhand = tf.reshape(tf.sparse.to_dense(x[\"lhand\"]), (-1, 21, 3))\n",
    "    rpose = tf.reshape(tf.sparse.to_dense(x[\"rpose\"]), (-1, 5, 3))\n",
    "    lpose = tf.reshape(tf.sparse.to_dense(x[\"lpose\"]), (-1, 5, 3))\n",
    "    phrase = tf.sparse.to_dense(x[\"phrase\"])\n",
    "\n",
    "    return face, rhand, lhand, rpose, lpose, phrase\n",
    "\n",
    "def pre_process_fn(lip, rhand, lhand, rpose, lpose, phrase):\n",
    "    phrase = tf.pad(phrase, [[0, MAX_PHRASE_LENGTH-tf.shape(phrase)[0]]], constant_values=pad_token_idx)\n",
    "    return pre_process1(lip, rhand, lhand, rpose, lpose), phrase\n",
    "\n",
    "MAX_PHRASE_LENGTH = 500\n",
    "tffiles = [f\"C:/Users/chuqi/ac215/capy_data_test/test_tfds/{file_id}.tfrecord\" for file_id in os.listdir('C:/Users/chuqi/ac215/capy_data_test/test_npy')]\n",
    "val_len = 1#int(0.05 * len(pqfiles))\n",
    "train_batch_size = 32\n",
    "val_batch_size = 32\n",
    "\n",
    "train_dataset =  tf.data.TFRecordDataset(tffiles[val_len:]).prefetch(tf.data.AUTOTUNE).shuffle(5000).map(decode_fn, num_parallel_calls=tf.data.AUTOTUNE).map(pre_process_fn, num_parallel_calls=tf.data.AUTOTUNE).batch(train_batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "val_dataset =  tf.data.TFRecordDataset(tffiles[:val_len]).prefetch(tf.data.AUTOTUNE).map(decode_fn, num_parallel_calls=tf.data.AUTOTUNE).map(pre_process_fn, num_parallel_calls=tf.data.AUTOTUNE).batch(val_batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "batch = next(iter(val_dataset))\n",
    "batch[0].shape, batch[1].shape\n",
    "print(batch[0].shape)\n",
    "print(batch[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ac8498e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ECA(tf.keras.layers.Layer):\n",
    "    def __init__(self, kernel_size=5, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.supports_masking = True\n",
    "        self.kernel_size = kernel_size\n",
    "        self.conv = tf.keras.layers.Conv1D(1, kernel_size=kernel_size, strides=1, padding=\"same\", use_bias=False)\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        nn = tf.keras.layers.GlobalAveragePooling1D()(inputs, mask=mask)\n",
    "        nn = tf.expand_dims(nn, -1)\n",
    "        nn = self.conv(nn)\n",
    "        nn = tf.squeeze(nn, -1)\n",
    "        nn = tf.nn.sigmoid(nn)\n",
    "        nn = nn[:,None,:]\n",
    "        return inputs * nn\n",
    "\n",
    "class CausalDWConv1D(tf.keras.layers.Layer):\n",
    "    def __init__(self, \n",
    "        kernel_size=17,\n",
    "        dilation_rate=1,\n",
    "        use_bias=False,\n",
    "        depthwise_initializer='glorot_uniform',\n",
    "        name='', **kwargs):\n",
    "        super().__init__(name=name,**kwargs)\n",
    "        self.causal_pad = tf.keras.layers.ZeroPadding1D((dilation_rate*(kernel_size-1),0),name=name + '_pad')\n",
    "        self.dw_conv = tf.keras.layers.DepthwiseConv1D(\n",
    "                            kernel_size,\n",
    "                            strides=1,\n",
    "                            dilation_rate=dilation_rate,\n",
    "                            padding='valid',\n",
    "                            use_bias=use_bias,\n",
    "                            depthwise_initializer=depthwise_initializer,\n",
    "                            name=name + '_dwconv')\n",
    "        self.supports_masking = True\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = self.causal_pad(inputs)\n",
    "        x = self.dw_conv(x)\n",
    "        return x\n",
    "\n",
    "def Conv1DBlock(channel_size,\n",
    "          kernel_size,\n",
    "          dilation_rate=1,\n",
    "          drop_rate=0.0,\n",
    "          expand_ratio=2,\n",
    "          se_ratio=0.25,\n",
    "          activation='swish',\n",
    "          name=None):\n",
    "    '''\n",
    "    efficient conv1d block, @hoyso48\n",
    "    '''\n",
    "    if name is None:\n",
    "        name = str(tf.keras.backend.get_uid(\"mbblock\"))\n",
    "    # Expansion phase\n",
    "    def apply(inputs):\n",
    "        channels_in = tf.keras.backend.int_shape(inputs)[-1]\n",
    "        channels_expand = channels_in * expand_ratio\n",
    "\n",
    "        skip = inputs\n",
    "\n",
    "        x = tf.keras.layers.Dense(\n",
    "            channels_expand,\n",
    "            use_bias=True,\n",
    "            activation=activation,\n",
    "            name=name + '_expand_conv')(inputs)\n",
    "\n",
    "        # Depthwise Convolution\n",
    "        x = CausalDWConv1D(kernel_size,\n",
    "            dilation_rate=dilation_rate,\n",
    "            use_bias=False,\n",
    "            name=name + '_dwconv')(x)\n",
    "\n",
    "        x = tf.keras.layers.BatchNormalization(momentum=0.95, name=name + '_bn')(x)\n",
    "\n",
    "        x  = ECA()(x)\n",
    "\n",
    "        x = tf.keras.layers.Dense(\n",
    "            channel_size,\n",
    "            use_bias=True,\n",
    "            name=name + '_project_conv')(x)\n",
    "\n",
    "        if drop_rate > 0:\n",
    "            x = tf.keras.layers.Dropout(drop_rate, noise_shape=(None,1,1), name=name + '_drop')(x)\n",
    "\n",
    "        if (channels_in == channel_size):\n",
    "            x = tf.keras.layers.add([x, skip], name=name + '_add')\n",
    "        return x\n",
    "\n",
    "    return apply\n",
    "\n",
    "class MultiHeadSelfAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, dim=256, num_heads=4, dropout=0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.dim = dim\n",
    "        self.scale = self.dim ** -0.5\n",
    "        self.num_heads = num_heads\n",
    "        self.qkv = tf.keras.layers.Dense(3 * dim, use_bias=False)\n",
    "        self.drop1 = tf.keras.layers.Dropout(dropout)\n",
    "        self.proj = tf.keras.layers.Dense(dim, use_bias=False)\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        qkv = self.qkv(inputs)\n",
    "        qkv = tf.keras.layers.Permute((2, 1, 3))(tf.keras.layers.Reshape((-1, self.num_heads, self.dim * 3 // self.num_heads))(qkv))\n",
    "        q, k, v = tf.split(qkv, [self.dim // self.num_heads] * 3, axis=-1)\n",
    "\n",
    "        attn = tf.matmul(q, k, transpose_b=True) * self.scale\n",
    "\n",
    "        if mask is not None:\n",
    "            mask = mask[:, None, None, :]\n",
    "\n",
    "        attn = tf.keras.layers.Softmax(axis=-1)(attn, mask=mask)\n",
    "        attn = self.drop1(attn)\n",
    "\n",
    "        x = attn @ v\n",
    "        x = tf.keras.layers.Reshape((-1, self.dim))(tf.keras.layers.Permute((2, 1, 3))(x))\n",
    "        x = self.proj(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def TransformerBlock(dim=256, num_heads=6, expand=4, attn_dropout=0.2, drop_rate=0.2, activation='swish'):\n",
    "    def apply(inputs):\n",
    "        x = inputs\n",
    "        x = tf.keras.layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "        x = MultiHeadSelfAttention(dim=dim,num_heads=num_heads,dropout=attn_dropout)(x)\n",
    "        x = tf.keras.layers.Dropout(drop_rate, noise_shape=(None,1,1))(x)\n",
    "        x = tf.keras.layers.Add()([inputs, x])\n",
    "        attn_out = x\n",
    "\n",
    "        x = tf.keras.layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "        x = tf.keras.layers.Dense(dim*expand, use_bias=False, activation=activation)(x)\n",
    "        x = tf.keras.layers.Dense(dim, use_bias=False)(x)\n",
    "        x = tf.keras.layers.Dropout(drop_rate, noise_shape=(None,1,1))(x)\n",
    "        x = tf.keras.layers.Add()([attn_out, x])\n",
    "        return x\n",
    "    return apply\n",
    "\n",
    "def positional_encoding(maxlen, num_hid):\n",
    "        depth = num_hid/2\n",
    "        positions = tf.range(maxlen, dtype = tf.float32)[..., tf.newaxis]\n",
    "        depths = tf.range(depth, dtype = tf.float32)[np.newaxis, :]/depth\n",
    "        angle_rates = tf.math.divide(1, tf.math.pow(tf.cast(10000, tf.float32), depths))\n",
    "        angle_rads = tf.linalg.matmul(positions, angle_rates)\n",
    "        pos_encoding = tf.concat(\n",
    "          [tf.math.sin(angle_rads), tf.math.cos(angle_rads)],\n",
    "          axis=-1)\n",
    "        return pos_encoding\n",
    "def positional_encoding2(maxlen, num_hid):\n",
    "        depth = num_hid/2\n",
    "        positions = tf.range(maxlen, dtype = tf.float32)[..., tf.newaxis]\n",
    "        depths = tf.range(depth, dtype = tf.float32)[np.newaxis, :]/depth\n",
    "        angle_rates = tf.math.divide(1, tf.math.pow(tf.cast(10000, tf.float32), depths))\n",
    "        angle_rads = tf.linalg.matmul(positions, angle_rates)\n",
    "        pos_encoding = np.zeros((maxlen, num_hid))\n",
    "        pos_encoding[:, 0::2] = np.sin(angle_rads)\n",
    "        pos_encoding[:, 1::2] = np.cos(angle_rads)\n",
    "        return pos_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5f42a259",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CTCLoss(labels, logits):\n",
    "    label_length = tf.reduce_sum(tf.cast(labels != pad_token_idx, tf.int32), axis=-1)\n",
    "    logit_length = tf.ones(tf.shape(logits)[0], dtype=tf.int32) * tf.shape(logits)[1]\n",
    "    loss = tf.nn.ctc_loss(\n",
    "            labels=labels,\n",
    "            logits=logits,\n",
    "            label_length=label_length,\n",
    "            logit_length=logit_length,\n",
    "            blank_index=pad_token_idx,\n",
    "            logits_time_major=False\n",
    "        )\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "800ec126",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(dim = 384):\n",
    "    inp = tf.keras.Input(INPUT_SHAPE)\n",
    "    x = tf.keras.layers.Masking(mask_value=0.0)(inp)\n",
    "    x = tf.keras.layers.Dense(dim, use_bias=False, name='stem_conv')(x)\n",
    "    pe = tf.cast(positional_encoding(INPUT_SHAPE[0], dim), dtype=x.dtype)\n",
    "    x = x + pe\n",
    "    x = tf.keras.layers.BatchNormalization(momentum=0.95,name='stem_bn')(x)\n",
    "    num_blocks = 6\n",
    "    drop_rate  = 0.4\n",
    "    for i in range(num_blocks):\n",
    "        x = Conv1DBlock(dim, 11, drop_rate=drop_rate)(x)\n",
    "        x = Conv1DBlock(dim,  5, drop_rate=drop_rate)(x)\n",
    "        x = Conv1DBlock(dim,  3, drop_rate=drop_rate)(x)\n",
    "        x = TransformerBlock(dim, expand=2)(x)\n",
    "\n",
    "    x = tf.keras.layers.Dense(dim*2,activation='relu',name='top_conv')(x)\n",
    "    x = tf.keras.layers.Dropout(0.4)(x)\n",
    "    #x = LateDropout(0.7)(x)\n",
    "    x = tf.keras.layers.Dense(len(char_to_num),name='classifier')(x)\n",
    "\n",
    "    model = tf.keras.Model(inp, x)\n",
    "\n",
    "    loss = CTCLoss\n",
    "    \n",
    "    # Adam Optimizer\n",
    "    optimizer = tfa.optimizers.RectifiedAdam(sma_threshold=4)\n",
    "    optimizer = tfa.optimizers.Lookahead(optimizer, sync_period=5)\n",
    "\n",
    "    model.compile(loss=loss, optimizer=optimizer)\n",
    "\n",
    "    return model\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "model = get_model()\n",
    "model(batch[0])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "711ac992",
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_to_char_fn(y):\n",
    "    return [num_to_char.get(x, \"\") for x in y]\n",
    "\n",
    "@tf.function()\n",
    "def decode_phrase(pred):\n",
    "    x = tf.argmax(pred, axis=1)\n",
    "    diff = tf.not_equal(x[:-1], x[1:])\n",
    "    adjacent_indices = tf.where(diff)[:, 0]\n",
    "    x = tf.gather(x, adjacent_indices)\n",
    "    mask = x != pad_token_idx\n",
    "    x = tf.boolean_mask(x, mask, axis=0)\n",
    "    return x\n",
    "\n",
    "# A utility function to decode the output of the network\n",
    "def decode_batch_predictions(pred):\n",
    "    output_text = []\n",
    "    for result in pred:\n",
    "        result = \"\".join(num_to_char_fn(decode_phrase(result).numpy()))\n",
    "        output_text.append(result)\n",
    "    return output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a1500087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A callback class to output a few transcriptions during training\n",
    "class CallbackEval(tf.keras.callbacks.Callback):\n",
    "    \"\"\"Displays a batch of outputs after every epoch.\"\"\"\n",
    "\n",
    "    def __init__(self, dataset):\n",
    "        super().__init__()\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def on_epoch_end(self, epoch: int, logs=None):\n",
    "        model.save_weights(\"model.h5\")\n",
    "        predictions = []\n",
    "        targets = []\n",
    "        for batch in self.dataset:\n",
    "            X, y = batch\n",
    "            batch_predictions = model(X)\n",
    "            batch_predictions = decode_batch_predictions(batch_predictions)\n",
    "            predictions.extend(batch_predictions)\n",
    "            for label in y:\n",
    "                label = \"\".join(num_to_char_fn(label.numpy()))\n",
    "                targets.append(label)\n",
    "        print(\"-\" * 100)\n",
    "        # for i in np.random.randint(0, len(predictions), 2):\n",
    "        print(f\"Target    : {targets}\")\n",
    "        print(f\"Prediction: {predictions}, len: {len(predictions)}\")\n",
    "        print(\"-\" * 100)\n",
    "        \n",
    "#         for i in range(32):\n",
    "#             print(f\"Target    : {targets[i]}\")\n",
    "#             print(f\"Prediction: {predictions[i]}, len: {len(predictions[i])}\")\n",
    "#             print(\"-\" * 100)\n",
    "\n",
    "# Callback function to check transcription on the val set.\n",
    "validation_callback = CallbackEval(val_dataset.take(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cf75aee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 51\n",
    "N_WARMUP_EPOCHS = 10\n",
    "LR_MAX = 1e-3\n",
    "WD_RATIO = 0.05\n",
    "WARMUP_METHOD = \"exp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2033b99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def lrfn(current_step, num_warmup_steps, lr_max, num_cycles=0.50, num_training_steps=N_EPOCHS):\n",
    "    \n",
    "    if current_step < num_warmup_steps:\n",
    "        if WARMUP_METHOD == 'log':\n",
    "            return lr_max * 0.10 ** (num_warmup_steps - current_step)\n",
    "        else:\n",
    "            return lr_max * 2 ** -(num_warmup_steps - current_step)\n",
    "    else:\n",
    "        progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))\n",
    "\n",
    "        return max(0.0, 0.5 * (1.0 + math.cos(math.pi * float(num_cycles) * 2.0 * progress))) * lr_max\n",
    "    \n",
    "def plot_lr_schedule(lr_schedule, epochs):\n",
    "    fig = plt.figure(figsize=(20, 10))\n",
    "    plt.plot([None] + lr_schedule + [None])\n",
    "    # X Labels\n",
    "    x = np.arange(1, epochs + 1)\n",
    "    x_axis_labels = [i if epochs <= 40 or i % 5 == 0 or i == 1 else None for i in range(1, epochs + 1)]\n",
    "    plt.xlim([1, epochs])\n",
    "    plt.xticks(x, x_axis_labels) # set tick step to 1 and let x axis start at 1\n",
    "    \n",
    "    # Increase y-limit for better readability\n",
    "    plt.ylim([0, max(lr_schedule) * 1.1])\n",
    "    \n",
    "    # Title\n",
    "    schedule_info = f'start: {lr_schedule[0]:.1E}, max: {max(lr_schedule):.1E}, final: {lr_schedule[-1]:.1E}'\n",
    "    plt.title(f'Step Learning Rate Schedule, {schedule_info}', size=18, pad=12)\n",
    "    \n",
    "    # Plot Learning Rates\n",
    "    for x, val in enumerate(lr_schedule):\n",
    "        if epochs <= 40 or x % 5 == 0 or x is epochs - 1:\n",
    "            if x < len(lr_schedule) - 1:\n",
    "                if lr_schedule[x - 1] < val:\n",
    "                    ha = 'right'\n",
    "                else:\n",
    "                    ha = 'left'\n",
    "            elif x == 0:\n",
    "                ha = 'right'\n",
    "            else:\n",
    "                ha = 'left'\n",
    "            plt.plot(x + 1, val, 'o', color='black');\n",
    "            offset_y = (max(lr_schedule) - min(lr_schedule)) * 0.02\n",
    "            plt.annotate(f'{val:.1E}', xy=(x + 1, val + offset_y), size=12, ha=ha)\n",
    "    \n",
    "    plt.xlabel('Epoch', size=16, labelpad=5)\n",
    "    plt.ylabel('Learning Rate', size=16, labelpad=5)\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "# Learning rate for encoder\n",
    "LR_SCHEDULE = [lrfn(step, num_warmup_steps=N_WARMUP_EPOCHS, lr_max=LR_MAX, num_cycles=0.50) for step in range(N_EPOCHS)]\n",
    "# Plot Learning Rate Schedule\n",
    "plot_lr_schedule(LR_SCHEDULE, epochs=N_EPOCHS)\n",
    "# Learning Rate Callback\n",
    "lr_callback = tf.keras.callbacks.LearningRateScheduler(lambda step: LR_SCHEDULE[step], verbose=0)\n",
    "\n",
    "# Custom callback to update weight decay with learning rate\n",
    "class WeightDecayCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, wd_ratio=WD_RATIO):\n",
    "        self.step_counter = 0\n",
    "        self.wd_ratio = wd_ratio\n",
    "    \n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        model.optimizer.weight_decay = model.optimizer.learning_rate * self.wd_ratio\n",
    "        print(f'learning rate: {model.optimizer.learning_rate.numpy():.2e}, weight decay: {model.optimizer.weight_decay.numpy():.2e}')\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath='code/baseline2/out2/model.{epoch:05d}.keras',\n",
    "    save_weights_only=True,\n",
    "    period=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6a64d2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(r\"C:/Users/chuqi/ac215/kaggle-data/model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "34d394c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "#     epochs=N_EPOCHS,\n",
    "    epochs=1,\n",
    "    callbacks=[\n",
    "        checkpoint_callback,\n",
    "        validation_callback,\n",
    "        lr_callback,\n",
    "        WeightDecayCallback(),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd746750",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wandb.keras import WandbCallback, WandbMetricsLogger\n",
    "# Initialize a W&B run\n",
    "wandb.init(\n",
    "    project = 'capy-train',\n",
    "    config = {\n",
    "      \"learning_rate\": LR_MAX,\n",
    "      \"epochs\": 1,\n",
    "      \"batch_size\": 32,\n",
    "      \"model_name\": model.name\n",
    "    },\n",
    "    name = model.name\n",
    ")\n",
    "\n",
    "# Train model\n",
    "# start_time = time.time()\n",
    "training_results = model.fit(\n",
    "        train_dataset,\n",
    "        validation_data=val_dataset,\n",
    "        epochs=1,\n",
    "        callbacks=[WandbCallback()],\n",
    "        #callbacks = [WandbMetricsLogger(log_freq=1)],\n",
    "        verbose=1)\n",
    "# execution_time = (time.time() - start_time)/60.0\n",
    "# print(\"Training execution time (mins)\",execution_time)\n",
    "\n",
    "# Update W&B\n",
    "# wandb.config.update({\"execution_time\": execution_time})\n",
    "# Close the W&B run\n",
    "wandb.run.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
